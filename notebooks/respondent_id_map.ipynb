{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d7f81f-e654-46bf-b9da-020066c31888",
   "metadata": {},
   "source": [
    "# Notebook to map respondent ID to entity ID\n",
    "\n",
    "This notebook provides a simple \"first draft\" of mapping of `respondent_id`'s from the historical\n",
    "DBF based FERC data to `entity_id`'s in the new XBRL based data. To do this, this notebook\n",
    "will use the years of data that FERC has migrated to the new XBRL format. Each filing they\n",
    "have migrated contains the `respondent_id` in the file name, and the `entity_id` embedded in\n",
    "the filings.\n",
    "\n",
    "The first step is to extract the migrated filings to a SQLite database to make the entity ID's\n",
    "accessible. These filings can be downloaded [here](https://ferc.gov/filing-forms/eforms-refresh/migrated-data-downloads).\n",
    "There's not data for every filer included in each year of data, so using the entire set of years will\n",
    "provide the best results. To create the SQLite database, extract the downloaded zip files to a single\n",
    "directory, then use the FERC XBRL extractor tool with the following command:\n",
    "\n",
    "```\n",
    "xbrl_extract {path_to_filing_directory} ferc1.sqlite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d162b1-0ef8-4387-b128-977574b48e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sa\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e7814-85af-42dc-bf26-e0189fde3d83",
   "metadata": {},
   "source": [
    "The only data needed to perform the mapping are the `filing_name`, and `entity_id` columns from the `identificiation_001_duration` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4716ae-7534-4758-be03-cfad971653de",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(\"sqlite:///ferc1.sqlite\")\n",
    "\n",
    "# Select RespondentLegalName as well for convenience\n",
    "id_table = pd.read_sql(\n",
    "    \"SELECT filing_name, entity_id, RespondentLegalName FROM identification_001_duration\",\n",
    "    engine,\n",
    "    parse_dates=[\"start_date\", \"end_date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09124f7d-37af-491a-92ef-a2c37f9dcaf2",
   "metadata": {},
   "source": [
    "The `respondent_id` is embedded in each filing name with the format `{UtilityName}-{respondent_id}-{year}{quarter}{form_number}`.\n",
    "The first step is extract that ID, then drop duplicate pairs (same pairs will exist for different years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a139a94-938c-4c85-957f-6e8244f6dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table[\"respondent_id\"] = pd.to_numeric(\n",
    "    id_table[\"filing_name\"].str.extract(r'.+-(\\d+)-.+').loc[:,0]\n",
    ")\n",
    "map_table = (\n",
    "    id_table.drop(\"filing_name\", axis=1)\n",
    "    .drop_duplicates(subset=[\"entity_id\", \"respondent_id\"])\n",
    "    .convert_dtypes()\n",
    "    .sort_values(by=[\"respondent_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd2d49-25b0-4e75-89c1-c9ec2f1aa069",
   "metadata": {},
   "source": [
    "Save the mapping to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ed2a1-950c-460f-ace0-0b2877b8f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table.to_csv(\"respondent_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d723c-635f-4d11-af6b-8f4956d89352",
   "metadata": {},
   "source": [
    "Check for any `entity_id`'s that map to multiple `respondent_id`'s. These will need to be analyzed\n",
    "further to identify if there are any mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26160cc7-d6e1-4878-8338-f51299c0568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table.loc[map_table[\"entity_id\"].duplicated(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d881a-2b10-4128-9f2d-6f8a23b80a6b",
   "metadata": {},
   "source": [
    "## Supplement missing `respondent_id`'s with string matching\n",
    "There are 257 unique `respondent_id -> entity_id` pairs. It's hard to say exactly how many there *should* be, because the set of unique `respondent_id`'s varies a fair amount from year to year. FERC has indicated that not all filings are included in the migrated data, so it is likely that there are missing `respondent_id`'s. To attempt to supplement this mapping, we can try to use the respondent legal names to try and find missing pairs. First, get all `respondent_id`'s from the raw DBF database produced by `ferc1_to_sqlite` in PUDL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69d2e0-c65c-489f-9c96-03124ce830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ferc1_db_path = \"/home/zach/catalyst/workspace/sqlite/ferc1.sqlite\"\n",
    "pudl_engine = sa.create_engine(\"sqlite:///\" + ferc1_db_path)\n",
    "inspector = sa.inspect(pudl_engine)\n",
    "\n",
    "# Get all respondent_id's by report_year\n",
    "df = pd.DataFrame(columns=[\"report_year\", \"respondent_id\"])\n",
    "for tbl in inspector.get_table_names():\n",
    "    cols = [col['name'] for col in inspector.get_columns(tbl)]\n",
    "    if (\"report_year\" in cols) & (\"respondent_id\" in cols):\n",
    "        df = pd.concat([df, pd.read_sql(f\"SELECT DISTINCT report_year, respondent_id FROM {tbl}\", pudl_engine)])\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Find respondent_id's that don't exist in migrated XBRL data\n",
    "missing_ids = set(df[\"respondent_id\"]) - set(map_table[\"respondent_id\"])\n",
    "print(len(missing_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe1f45-9113-4d50-af29-a76de2788f0c",
   "metadata": {},
   "source": [
    "There are 116 ID's that are missing in the XBRL data, which can certainly be mapped manually, but it might be easier to try and use some fuzzy string matching to help out. There's a table provided by FERC that maps between the new `entity_id`'s and respondent names, which can be downloaded [here](https://www.ferc.gov/media/ferc-cid-listing). This will be used to do the string matching. Many of the missing ID's also don't appear at all in newer years of data, and quite possibly don't exist in FERC's new Company Identifier system, so it will likely only be a subset of the missing ID's for which there is a match at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3905c4-c19b-4635-880d-b5f7c250a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CID lookup table\n",
    "lookup_table = pd.read_excel(\"FERC_CID_Listing_6-6-2022.xlsx\", skiprows=2)\n",
    "lookup_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723a5cf-bb0c-4f4d-bec3-84e6e915726a",
   "metadata": {},
   "source": [
    "For each missing `respondent_id` find the respondent name that most closely matches the name listed in the DBF database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49096a0-79b7-45ff-b95d-2326e4082221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read respondent ID table from historical data for respondent names\n",
    "respondent_names = pd.read_sql(\"SELECT respondent_id, respondent_name FROM f1_respondent_id\", pudl_engine)\n",
    "respondent_names = respondent_names.set_index(\"respondent_id\")\n",
    "\n",
    "# Loop through missing IDs and do a fuzzy string match between respondent names and CID lookup table names\n",
    "new_matches_df = {\"entity_id\": [], \"RespondentLegalName\": [], \"respondent_id\": [], \"old_name\": []}\n",
    "for r_id in missing_ids:\n",
    "    name = respondent_names[\"respondent_name\"][r_id]\n",
    "    matches = lookup_table[\"Organization Name\"].apply(fuzz.ratio, args=(name,))\n",
    "    \n",
    "    if matches.max() > 85:\n",
    "        # Add row to new dataframe\n",
    "        new_matches_df[\"entity_id\"].append(lookup_table[\"CID\"][matches.idxmax()])\n",
    "        new_matches_df[\"respondent_id\"].append(r_id)\n",
    "        new_matches_df[\"RespondentLegalName\"].append(lookup_table[\"Organization Name\"][matches.idxmax()])\n",
    "        new_matches_df[\"old_name\"].append(name)\n",
    "\n",
    "        \n",
    "        # Print matches for visual inspection\n",
    "        print(f\"{name} -> {lookup_table['Organization Name'][matches.idxmax()]}\")\n",
    "\n",
    "new_matches_df = pd.DataFrame(new_matches_df).set_index(\"old_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec00538-bea3-49b5-93d6-107b9f6dd88b",
   "metadata": {},
   "source": [
    "I've limited the matches to only those with a fuzzy match ratio greater than 85. This is somewhat arbitrary, but I settled on this by lowering it until all additional matches found were clearly false positives. There are clearly several false positives that need to be manually removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a82fa9-9c9f-425e-9f4a-efb8aa161865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positives from visual inspection\n",
    "false_positives = [\n",
    "    \"IES Utilities Inc.\",\n",
    "    \"TXE Electric Company\",\n",
    "    \"Bridger Valley Electric Association, Inc.\",\n",
    "    \"Flowell Electric Association, Inc.\",\n",
    "]\n",
    "\n",
    "# The top match for 'Ocean State Power' was 'Ocean State Power II', so I'll manually map this to 'Ocean State Power LLC'\n",
    "new_matches_df.loc[\"Ocean State Power\"] = {\"RespondentLegalName\": \"Ocean State Power LLC\", \"entity_id\": \"C001906\", \"respondent_id\": 124\"}\n",
    "\n",
    "# Drop false positives then get rid of old_name column\n",
    "new_matches_df = new_matches_df.drop(\n",
    "    new_matches_df[new_matches_df[\"old_name\"].isin(false_positives)].index\n",
    ")\n",
    "new_matches_df = new_matches_df.drop(\"old_name\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a40658-92cd-43c1-9bc5-f9ba5233ac62",
   "metadata": {},
   "source": [
    "Now these new matches can be added to the mapping table and the CSV can be rewritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361699bf-4b79-4345-a6bf-af467fa2256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table = pd.concat([map_table, new_matches_df])\n",
    "map_table.to_csv(\"respondent_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621359f-521b-483e-9b7e-fc3247a03955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
